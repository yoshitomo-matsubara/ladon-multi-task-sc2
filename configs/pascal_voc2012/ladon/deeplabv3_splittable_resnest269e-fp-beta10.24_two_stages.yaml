dependencies:
  - name: 'custom'

datasets:
  &coco_train coco2017/train: &coco_train_dataset !import_call
    _name: &dataset_name1 'coco2017'
    _root: &root_dir1 !join ['~/datasets/', *dataset_name1]
    key: 'coco.dataset.coco_dataset'
    init:
      kwargs:
        img_dir_path: !join [*root_dir1, '/train2017']
        ann_file_path: !join [*root_dir1, '/annotations/instances_train2017.json']
        annotated_only: True
        is_segment: True
        transforms: !import_call
          key: 'custom.transform.CustomCompose'
          init:
            kwargs:
              transforms:
                - !import_call
                  key: 'custom.transform.CustomRandomResize'
                  init:
                    kwargs: &train_random_resize_kwargs
                      min_size: 260
                      max_size: 832
                      square: True
                - !import_call
                  key: 'custom.transform.CustomRandomHorizontalFlip'
                  init:
                    kwargs:
                      p: 0.5
                - !import_call
                  key: 'custom.transform.CustomRandomCrop'
                  init:
                    kwargs: &final_size
                      size: 416
                - !import_call
                  key: 'custom.transform.CustomToTensor'
                  init:
                - !import_call
                  key: 'custom.transform.CustomNormalize'
                  init:
                    kwargs: &normalize_kwargs
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]

  &coco_val coco2017/val: !import_call
    key: 'coco.dataset.coco_dataset'
    init:
      kwargs:
        img_dir_path: !join [ *root_dir1, '/val2017' ]
        ann_file_path: !join [ *root_dir1, '/annotations/instances_val2017.json' ]
        annotated_only: False
        is_segment: True
        transforms: !import_call
          key: 'custom.transform.CustomCompose'
          init:
            kwargs:
              transforms:
                - !import_call
                  key: 'custom.transform.CustomRandomResize'
                  init:
                    kwargs:
                      min_size: *final_size
                      max_size: *final_size
                      square: True
                - !import_call
                  key: 'custom.transform.CustomToTensor'
                  init:
                - !import_call
                  key: 'custom.transform.CustomNormalize'
                  init:
                    kwargs: *normalize_kwargs
  &pascal_train 'pascal_voc2012/train': &pascal_train_dataset !import_call
    _name: &dataset_name2 'pascal_voc2012'
    _root: &root_dir2 '~/datasets'
    key: 'torchvision.datasets.VOCSegmentation'
    init:
      kwargs:
        root: *root_dir2
        image_set: 'train'
        year: '2012'
        download: True
        transforms: !import_call
          key: 'custom.transform.CustomCompose'
          init:
            kwargs:
              transforms:
                - !import_call
                  key: 'custom.transform.CustomRandomResize'
                  init:
                    kwargs: *train_random_resize_kwargs
                - !import_call
                  key: 'custom.transform.CustomRandomHorizontalFlip'
                  init:
                    kwargs:
                      p: 0.5
                - !import_call
                  key: 'custom.transform.CustomRandomCrop'
                  init:
                    kwargs:
                      size: *final_size
                - !import_call
                  key: 'custom.transform.CustomToTensor'
                  init:
                - !import_call
                  key: 'custom.transform.CustomNormalize'
                  init:
                    kwargs: *normalize_kwargs
  &pascal_val 'pascal_voc2012/val': !import_call
    key: 'torchvision.datasets.VOCSegmentation'
    init:
      kwargs:
        root: *root_dir2
        image_set: 'val'
        year: '2012'
        download: True
        transforms: !import_call
          key: 'custom.transform.CustomCompose'
          init:
            kwargs:
              transforms:
                - !import_call
                  key: 'custom.transform.CustomRandomResize'
                  init:
                    kwargs:
                      min_size: *final_size
                      max_size: *final_size
                      square: True
                - !import_call
                  key: 'sc2bench.transforms.misc.CustomToTensor'
                  init:
                    kwargs:
                      converts_sample: True
                      converts_target: True
                - !import_call
                  key: 'custom.transform.CustomNormalize'
                  init:
                    kwargs: *normalize_kwargs

models:
  model:
    key: 'deeplabv3_model'
    kwargs:
      pretrained: False
      pretrained_backbone_name:
      num_classes: 21
      uses_aux: True
      num_input_channels: 2048
      num_aux_channels: 1024
      return_layer_dict:
        layer3: 'aux'
        layer4: 'out'
      analysis_config:
        analyzes_after_compress: True
        analyzer_configs:
          - key: 'FileSizeAnalyzer'
            kwargs:
              unit: 'KB'
      analyzable_layer_key: 'bottleneck_layer'
      backbone_config:
        key: 'splittable_resnest'
        kwargs:
          num_classes: 1000
          pretrained: False
          inplanes: 2048
          bottleneck_config:
            key: 'FPBasedResNetBottleneck'
            kwargs:
              num_bottleneck_channels: 32
              num_target_channels: 256
          resnest_name: 'resnest269e'
          pre_transform:
          skips_avgpool: True
          skips_fc: True
        src_ckpt: './resource/ckpt/ilsvrc2012/ladon/ilsvrc2012-splittable_resnest269e-fp-beta10.24_from_resnest269e.pt'
      start_ckpt_file_path:
    experiment: &student_experiment !join [*dataset_name2, '-deeplabv3_splittable_resnest269e-fp-beta10.24_two_stages']
    dst_ckpt: !join ['./resource/ckpt/pascal_voc2012/ladon/', *student_experiment, '.pt']

train:
  log_freq: 1000
  epoch_to_update: 0
  stage1:
    num_epochs: &coco_num_epochs 30
    train_data_loader:
      dataset_id: *coco_train
      sampler:
        class_or_func: !import_get
          key: 'torch.utils.data.RandomSampler'
        kwargs:
      collate_fn: 'coco_seg_collate_fn'
      kwargs:
        batch_size: &train_batch_size1 16
        num_workers: 16
      cache_output:
    val_data_loader:
      dataset_id: *coco_val
      sampler:
        class_or_func: !import_get
          key: 'torch.utils.data.SequentialSampler'
        kwargs:
      collate_fn: 'coco_seg_eval_collate_fn'
      kwargs:
        batch_size: 1
        num_workers: 16
    model:
      adaptations:
      sequential: []
      frozen_modules: ['backbone']
      forward_proc: 'forward_batch_only'
      forward_hook:
        input: []
        output: []
      wrapper: 'DistributedDataParallel'
      requires_grad: True
    optimizer:
      key: 'SGD'
      kwargs:
        lr: 0.02
        momentum: 0.9
        weight_decay: 0.0001
      module_wise_kwargs:
        - module: 'backbone'
          kwargs: {}
        - module: 'classifier'
          kwargs: {}
        - module: 'aux_classifier'
          kwargs:
            lr: 0.01
    scheduler:
      key: 'poly_lr_scheduler'
      kwargs:
        num_iterations: !import_call
          key: 'utils.dataset.get_num_iterations'
          init:
            kwargs:
              dataset: *pascal_train_dataset
              batch_size: *train_batch_size1
              world_size: 1
        num_epochs: *coco_num_epochs
        power: 0.9
      scheduling_step: 1
    criterion: &ce_loss
      key: 'WeightedSumLoss'
      kwargs:
        sub_terms:
          ce:
            criterion:
              key: 'CrossEntropyLoss'
              kwargs:
                reduction: 'mean'
                ignore_index: 255
            criterion_wrapper:
              key: 'DictLossWrapper'
              kwargs:
                input:
                  is_from_teacher: False
                  module_path: '.'
                  io: 'output'
                target:
                  uses_label: True
                weights:
                  out: 1.0
                  aux: 0.5
            weight: 1.0
  stage2:
    num_epochs: &pascal_num_epochs 60
    train_data_loader:
      dataset_id: *pascal_train
      sampler:
        class_or_func: !import_get
          key: 'torch.utils.data.RandomSampler'
        kwargs:
      collate_fn: 'pascal_seg_collate_fn'
      kwargs:
        batch_size: &train_batch_size2 16
        num_workers: 16
      cache_output:
    val_data_loader:
      dataset_id: *pascal_val
      sampler:
        class_or_func: !import_get
          key: 'torch.utils.data.SequentialSampler'
        kwargs:
      collate_fn: 'pascal_seg_eval_collate_fn'
      kwargs:
        batch_size: 1
        num_workers: 16
    model:
      adaptations:
      sequential: []
      frozen_modules: ['backbone']
      forward_proc: 'forward_batch_only'
      forward_hook:
        input: []
        output: []
      wrapper: 'DistributedDataParallel'
      requires_grad: True
    optimizer:
      key: 'SGD'
      kwargs:
        lr: 0.02
        momentum: 0.9
        weight_decay: 0.0001
      module_wise_kwargs:
        - module: 'backbone'
          kwargs: {}
        - module: 'classifier'
          kwargs: {}
        - module: 'aux_classifier'
          kwargs:
            lr: 0.01
    scheduler:
      key: 'poly_lr_scheduler'
      kwargs:
        num_iterations: !import_call
          key: 'utils.dataset.get_num_iterations'
          init:
            kwargs:
              dataset: *pascal_train_dataset
              batch_size: *train_batch_size2
              world_size: 1
        num_epochs: *pascal_num_epochs
        power: 0.9
      scheduling_step: 1
    criterion: *ce_loss

test:
  test_data_loader:
    dataset_id: *pascal_val
    sampler:
      class_or_func: !import_get
        key: 'torch.utils.data.SequentialSampler'
      kwargs:
    collate_fn: 'pascal_seg_eval_collate_fn'
    kwargs:
      batch_size: 1
      num_workers: 16
